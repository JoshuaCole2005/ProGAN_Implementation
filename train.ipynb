{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from model import Discriminator, Generator\n",
    "from math import log2\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from scipy.stats import truncnorm\n",
    "\n",
    "torch.backends.cudnn.benchmarks = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "starting_img_size = 4\n",
    "dataset = \"celeb_hq_dataset\"\n",
    "checkpoint_generator = \"generator.pth\"\n",
    "checkpoint_discriminator = \"discriminator.pth\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "lr = 1e-3\n",
    "batch_size = [4, 4, 4, 4, 4, 4, 4, 2, 1]\n",
    "img_size = 512\n",
    "img_channels = 3\n",
    "z_dim = 256\n",
    "in_channels = 256\n",
    "l_gp = 10\n",
    "num_steps = int(log2(img_size /4)) + 1\n",
    "progressive_epochs = [10] * len(batch_size)\n",
    "noise = torch.randn(8, z_dim, 1, 1).to(device)\n",
    "workers = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extra Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_penalty(discriminator, real, fake, alpha, train_step, device=\"cpu\"):\n",
    "    BATCH_SIZE, C, H, W = real.shape\n",
    "    beta = torch.rand((BATCH_SIZE, 1, 1, 1)).repeat(1, C, H, W).to(device)\n",
    "    interpolated_images = real * beta + fake.detach() * (1 - beta)\n",
    "    interpolated_images.requires_grad_(True)\n",
    "    mixed_scores = discriminator(interpolated_images, alpha, train_step)\n",
    "    gradient = torch.autograd.grad(inputs=interpolated_images, outputs=mixed_scores, grad_outputs=torch.ones_like(mixed_scores), create_graph=True, retain_graph=True,)[0]\n",
    "    gradient = gradient.view(gradient.shape[0], -1)\n",
    "    gradient_norm = gradient.norm(2, dim=1)\n",
    "    gradient_penalty = torch.mean((gradient_norm - 1) ** 2)\n",
    "    return gradient_penalty\n",
    "\n",
    "def save_checkpoint(model, optimizer, filename=\"my_checkpoint.pth.tar\"):\n",
    "    print(\"=> Saving checkpoint\")\n",
    "    checkpoint = {\"state_dict\": model.state_dict(),\"optimizer\": optimizer.state_dict(),}\n",
    "    torch.save(checkpoint, filename)\n",
    "\n",
    "\n",
    "def load_checkpoint(checkpoint_file, model, optimizer, lr):\n",
    "    print(\"=> Loading checkpoint\")\n",
    "    checkpoint = torch.load(checkpoint_file, map_location=\"cuda\")\n",
    "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group[\"lr\"] = lr\n",
    "\n",
    "def plot_to_tensorboard(writer, disc_loss, loss_gen, real, fake, tensorboard_step):\n",
    "    writer.add_scalar(\"Loss discriminator\", disc_loss, global_step=tensorboard_step)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        img_grid_real = torchvision.utils.make_grid(real[:8], normalize=True)\n",
    "        img_grid_fake = torchvision.utils.make_grid(fake[:8], normalize=True)\n",
    "        writer.add_image(\"Real\", img_grid_real, global_step=tensorboard_step)\n",
    "        writer.add_image(\"Fake\", img_grid_fake, global_step=tensorboard_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_loader(image_size):\n",
    "    transform = transforms.Compose([transforms.Resize((image_size, image_size)), transforms.ToTensor(), transforms.RandomHorizontalFlip(p=0.5), transforms.Normalize( [0.5 for _ in range(img_channels)], [0.5 for _ in range(img_channels)], ), ] )\n",
    "    batch_sizes = batch_size[int(log2(image_size / 4))]\n",
    "    data = datasets.ImageFolder(root=dataset, transform=transform)\n",
    "    loader = DataLoader(data, batch_size=batch_sizes, shuffle=True, num_workers=workers, pin_memory=True,)\n",
    "    return loader, data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(discriminator, gen, loader, dataset, step, alpha, discriminator_optimizer, opt_gen, tensorboard_step, writer, generator_Scaler, discriminator_Scaler,):\n",
    "    loop = tqdm(loader, leave=True)\n",
    "    for batch_idx, (real, _) in enumerate(loop):\n",
    "        real = real.to(device)\n",
    "        cur_batch_size = real.shape[0]\n",
    "\n",
    "        noise = torch.randn(cur_batch_size, z_dim, 1, 1).to(device)\n",
    "\n",
    "        with torch.cuda.amp.autocast():\n",
    "            fake = gen(noise, alpha, step)\n",
    "            discriminator_real = discriminator(real, alpha, step)\n",
    "            discriminator_fake = discriminator(fake.detach(), alpha, step)\n",
    "            gp = gradient_penalty(discriminator, real, fake, alpha, step, device=device)\n",
    "            disc_loss = (-(torch.mean(discriminator_real) - torch.mean(discriminator_fake)) + l_gp * gp + (0.001 * torch.mean(discriminator_real ** 2)) )\n",
    "\n",
    "        discriminator_optimizer.zero_grad()\n",
    "        discriminator_Scaler.scale(disc_loss).backward()\n",
    "        discriminator_Scaler.step(discriminator_optimizer)\n",
    "        discriminator_Scaler.update()\n",
    "\n",
    "        with torch.cuda.amp.autocast():\n",
    "            gen_fake = discriminator(fake, alpha, step)\n",
    "            loss_gen = -torch.mean(gen_fake)\n",
    "\n",
    "        opt_gen.zero_grad()\n",
    "        generator_Scaler.scale(loss_gen).backward()\n",
    "        generator_Scaler.step(opt_gen)\n",
    "        generator_Scaler.update()\n",
    "\n",
    "        alpha += cur_batch_size / (\n",
    "            (progressive_epochs[step] * 0.5) * len(dataset)\n",
    "        )\n",
    "        alpha = min(alpha, 1)\n",
    "\n",
    "        if batch_idx % 500 == 0:\n",
    "            with torch.no_grad():\n",
    "                fixed_fakes = gen(noise, alpha, step) * 0.5 + 0.5\n",
    "            plot_to_tensorboard(\n",
    "                writer,\n",
    "                disc_loss.item(),\n",
    "                loss_gen.item(),\n",
    "                real.detach(),\n",
    "                fixed_fakes.detach(),\n",
    "                tensorboard_step,\n",
    "            )\n",
    "            tensorboard_step += 1\n",
    "\n",
    "        loop.set_postfix(\n",
    "            gp=gp.item(),\n",
    "            disc_loss=disc_loss.item(),\n",
    "        )\n",
    "\n",
    "    return tensorboard_step, alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    gen = Generator(z_dim, in_channels, img_channels=img_channels).to(device)\n",
    "    discriminator = Discriminator(z_dim, in_channels, img_channels=img_channels).to(device)\n",
    "    generator_optimizer = optim.Adam(gen.parameters(), lr=lr, betas=(0.0, 0.99))\n",
    "    discriminator_optimizer = optim.Adam(discriminator.parameters(), lr=lr, betas=(0.0, 0.99))\n",
    "    discriminator_scaler = torch.cuda.amp.GradScaler()\n",
    "    generator_scaler = torch.cuda.amp.GradScaler()\n",
    "    writer = SummaryWriter(f\"logs/true_GAN\")\n",
    "    gen.train()\n",
    "    discriminator.train()\n",
    "\n",
    "    tensorboard_step = 0\n",
    "    step = int(log2(starting_img_size / 4))\n",
    "    for num_epochs in progressive_epochs[step:]:\n",
    "        alpha = 0\n",
    "        loader, dataset = create_loader(4 * 2 ** step)\n",
    "        print(f\"Current image size: {4 * 2 ** step}\")\n",
    "        print(\"----------------------------------------------------------------------------------\")\n",
    "        for epoch in range(num_epochs):\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}]\")\n",
    "            tensorboard_step, alpha = train(discriminator,gen, loader, dataset, step, alpha, discriminator_optimizer, generator_optimizer, tensorboard_step, writer, generator_scaler, discriminator_scaler,)\n",
    "            save_checkpoint(gen, generator_optimizer, filename=checkpoint_generator)\n",
    "            save_checkpoint(discriminator, discriminator_optimizer, filename=checkpoint_discriminator)\n",
    "\n",
    "        step += 1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current image size: 4\n",
      "----------------------------------------------------------------------------------\n",
      "Epoch [1/10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4486/4486 [01:39<00:00, 45.04it/s, disc_loss=0.251, gp=0.00156]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [2/10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4486/4486 [01:37<00:00, 45.97it/s, disc_loss=0.422, gp=0.00296]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [3/10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4486/4486 [01:39<00:00, 45.30it/s, disc_loss=-.12, gp=0.00756]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [4/10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4486/4486 [01:37<00:00, 45.85it/s, disc_loss=0.219, gp=0.00701]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [5/10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4486/4486 [01:36<00:00, 46.25it/s, disc_loss=-.199, gp=0.0113]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [6/10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4486/4486 [01:37<00:00, 45.78it/s, disc_loss=0.271, gp=0.00539]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [7/10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4486/4486 [01:38<00:00, 45.47it/s, disc_loss=-.496, gp=0.0225]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [8/10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4486/4486 [01:38<00:00, 45.67it/s, disc_loss=0.203, gp=0.00322]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [9/10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4486/4486 [01:36<00:00, 46.40it/s, disc_loss=-.000205, gp=0.0164] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [10/10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4486/4486 [01:37<00:00, 46.09it/s, disc_loss=0.261, gp=0.0182]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Current image size: 8\n",
      "----------------------------------------------------------------------------------\n",
      "Epoch [1/10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4486/4486 [02:54<00:00, 25.73it/s, disc_loss=0.638, gp=0.00525]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [2/10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4486/4486 [02:53<00:00, 25.89it/s, disc_loss=-.499, gp=0.00598]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [3/10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4486/4486 [02:52<00:00, 25.96it/s, disc_loss=0.706, gp=0.0108]     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [4/10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4486/4486 [02:53<00:00, 25.93it/s, disc_loss=-.204, gp=0.0116]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [5/10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4486/4486 [02:58<00:00, 25.19it/s, disc_loss=0.731, gp=0.0186]     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [6/10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4486/4486 [02:55<00:00, 25.61it/s, disc_loss=0.357, gp=0.00563]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [7/10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4486/4486 [02:56<00:00, 25.45it/s, disc_loss=-.133, gp=0.0575]     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [8/10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4486/4486 [02:54<00:00, 25.73it/s, disc_loss=0.367, gp=0.00735]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [9/10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4486/4486 [02:56<00:00, 25.47it/s, disc_loss=-.434, gp=0.00436]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [10/10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4486/4486 [02:54<00:00, 25.68it/s, disc_loss=1.11, gp=0.00763]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Current image size: 16\n",
      "----------------------------------------------------------------------------------\n",
      "Epoch [1/10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4486/4486 [03:37<00:00, 20.60it/s, disc_loss=-.457, gp=0.0358]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [2/10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4486/4486 [03:42<00:00, 20.18it/s, disc_loss=0.919, gp=0.021]     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [3/10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4486/4486 [03:44<00:00, 20.02it/s, disc_loss=-1.21, gp=0.0102]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [4/10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4486/4486 [03:41<00:00, 20.27it/s, disc_loss=0.995, gp=0.00622]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [5/10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4486/4486 [03:40<00:00, 20.35it/s, disc_loss=0.479, gp=0.0156]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [6/10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4486/4486 [03:43<00:00, 20.08it/s, disc_loss=-.805, gp=0.00415]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [7/10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4486/4486 [03:45<00:00, 19.91it/s, disc_loss=0.237, gp=0.0126]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [8/10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4486/4486 [03:42<00:00, 20.21it/s, disc_loss=-.0692, gp=5.2e-5]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [9/10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4486/4486 [03:38<00:00, 20.49it/s, disc_loss=0.689, gp=0.00524]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [10/10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4486/4486 [03:41<00:00, 20.25it/s, disc_loss=0.4, gp=0.00125]     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Current image size: 32\n",
      "----------------------------------------------------------------------------------\n",
      "Epoch [1/10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4486/4486 [04:41<00:00, 15.92it/s, disc_loss=-1.28, gp=0.0155]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [2/10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4486/4486 [04:36<00:00, 16.22it/s, disc_loss=1.2, gp=0.0411]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [3/10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4486/4486 [04:36<00:00, 16.24it/s, disc_loss=-.785, gp=0.00389]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [4/10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4486/4486 [04:37<00:00, 16.18it/s, disc_loss=0.241, gp=0.00898]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [5/10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4486/4486 [04:42<00:00, 15.89it/s, disc_loss=-1.02, gp=0.00709]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [6/10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4486/4486 [04:39<00:00, 16.05it/s, disc_loss=-3.32, gp=0.00193]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [7/10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4486/4486 [04:37<00:00, 16.16it/s, disc_loss=0.513, gp=0.00181]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [8/10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4486/4486 [04:39<00:00, 16.06it/s, disc_loss=-1.7, gp=0.0495]     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [9/10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4486/4486 [04:43<00:00, 15.82it/s, disc_loss=-.0406, gp=0.00565]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [10/10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4486/4486 [04:40<00:00, 15.98it/s, disc_loss=0.298, gp=0.00548]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Current image size: 64\n",
      "----------------------------------------------------------------------------------\n",
      "Epoch [1/10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4486/4486 [07:49<00:00,  9.56it/s, disc_loss=-2.02, gp=0.0172]     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [2/10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4486/4486 [08:02<00:00,  9.30it/s, disc_loss=-2.3, gp=0.00285]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [3/10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 2942/4486 [05:26<02:51,  9.01it/s, disc_loss=-.545, gp=0.00362]  \n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Caught MemoryError in DataLoader worker process 2.\nOriginal Traceback (most recent call last):\n  File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\worker.py\", line 302, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 49, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 49, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torchvision\\datasets\\folder.py\", line 230, in __getitem__\n    sample = self.loader(path)\n  File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torchvision\\datasets\\folder.py\", line 269, in default_loader\n    return pil_loader(path)\n  File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torchvision\\datasets\\folder.py\", line 249, in pil_loader\n    return img.convert(\"RGB\")\n  File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\PIL\\Image.py\", line 901, in convert\n    return self.copy()\n  File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\PIL\\Image.py\", line 1126, in copy\n    return self._new(self.im.copy())\nMemoryError\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\joshu\\Python Projects\\Machine Learning Projects\\ProGAN From Scratch\\train.ipynb Cell 10\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/joshu/Python%20Projects/Machine%20Learning%20Projects/ProGAN%20From%20Scratch/train.ipynb#X12sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m main()\n",
      "\u001b[1;32mc:\\Users\\joshu\\Python Projects\\Machine Learning Projects\\ProGAN From Scratch\\train.ipynb Cell 10\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/joshu/Python%20Projects/Machine%20Learning%20Projects/ProGAN%20From%20Scratch/train.ipynb#X12sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_epochs):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/joshu/Python%20Projects/Machine%20Learning%20Projects/ProGAN%20From%20Scratch/train.ipynb#X12sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch [\u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mnum_epochs\u001b[39m}\u001b[39;00m\u001b[39m]\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/joshu/Python%20Projects/Machine%20Learning%20Projects/ProGAN%20From%20Scratch/train.ipynb#X12sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     tensorboard_step, alpha \u001b[39m=\u001b[39m train(discriminator,gen, loader, dataset, step, alpha, discriminator_optimizer, generator_optimizer, tensorboard_step, writer, generator_scaler, discriminator_scaler,)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/joshu/Python%20Projects/Machine%20Learning%20Projects/ProGAN%20From%20Scratch/train.ipynb#X12sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     save_checkpoint(gen, generator_optimizer, filename\u001b[39m=\u001b[39mcheckpoint_generator)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/joshu/Python%20Projects/Machine%20Learning%20Projects/ProGAN%20From%20Scratch/train.ipynb#X12sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     save_checkpoint(discriminator, discriminator_optimizer, filename\u001b[39m=\u001b[39mcheckpoint_discriminator)\n",
      "\u001b[1;32mc:\\Users\\joshu\\Python Projects\\Machine Learning Projects\\ProGAN From Scratch\\train.ipynb Cell 10\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(discriminator, gen, loader, dataset, step, alpha, discriminator_optimizer, opt_gen, tensorboard_step, writer, generator_Scaler, discriminator_Scaler)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/joshu/Python%20Projects/Machine%20Learning%20Projects/ProGAN%20From%20Scratch/train.ipynb#X12sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain\u001b[39m(discriminator, gen, loader, dataset, step, alpha, discriminator_optimizer, opt_gen, tensorboard_step, writer, generator_Scaler, discriminator_Scaler,):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/joshu/Python%20Projects/Machine%20Learning%20Projects/ProGAN%20From%20Scratch/train.ipynb#X12sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     loop \u001b[39m=\u001b[39m tqdm(loader, leave\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/joshu/Python%20Projects/Machine%20Learning%20Projects/ProGAN%20From%20Scratch/train.ipynb#X12sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39mfor\u001b[39;00m batch_idx, (real, _) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(loop):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/joshu/Python%20Projects/Machine%20Learning%20Projects/ProGAN%20From%20Scratch/train.ipynb#X12sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m         real \u001b[39m=\u001b[39m real\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/joshu/Python%20Projects/Machine%20Learning%20Projects/ProGAN%20From%20Scratch/train.ipynb#X12sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m         cur_batch_size \u001b[39m=\u001b[39m real\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\tqdm\\std.py:1195\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1192\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[0;32m   1194\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1195\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[0;32m   1196\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[0;32m   1197\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1198\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:652\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    649\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    650\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    651\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 652\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    653\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    654\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    655\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    656\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1347\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1345\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1346\u001b[0m     \u001b[39mdel\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_task_info[idx]\n\u001b[1;32m-> 1347\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_process_data(data)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1373\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m   1371\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_try_put_index()\n\u001b[0;32m   1372\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[1;32m-> 1373\u001b[0m     data\u001b[39m.\u001b[39;49mreraise()\n\u001b[0;32m   1374\u001b[0m \u001b[39mreturn\u001b[39;00m data\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\_utils.py:461\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    457\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m    458\u001b[0m     \u001b[39m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[0;32m    459\u001b[0m     \u001b[39m# instantiate since we don't know how to\u001b[39;00m\n\u001b[0;32m    460\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(msg) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m--> 461\u001b[0m \u001b[39mraise\u001b[39;00m exception\n",
      "\u001b[1;31mMemoryError\u001b[0m: Caught MemoryError in DataLoader worker process 2.\nOriginal Traceback (most recent call last):\n  File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\worker.py\", line 302, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 49, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 49, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torchvision\\datasets\\folder.py\", line 230, in __getitem__\n    sample = self.loader(path)\n  File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torchvision\\datasets\\folder.py\", line 269, in default_loader\n    return pil_loader(path)\n  File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\torchvision\\datasets\\folder.py\", line 249, in pil_loader\n    return img.convert(\"RGB\")\n  File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\PIL\\Image.py\", line 901, in convert\n    return self.copy()\n  File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\PIL\\Image.py\", line 1126, in copy\n    return self._new(self.im.copy())\nMemoryError\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
